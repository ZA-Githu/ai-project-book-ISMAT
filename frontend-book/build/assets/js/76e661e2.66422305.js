"use strict";(globalThis.webpackChunkbook_frontend=globalThis.webpackChunkbook_frontend||[]).push([[687],{6901(n,e,i){i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai-robotics/autonomous-humanoid-capstone","title":"Capstone - The Autonomous Humanoid","description":"Overview","source":"@site/docs/ai-robotics/autonomous-humanoid-capstone.md","sourceDirName":"ai-robotics","slug":"/ai-robotics/autonomous-humanoid-capstone","permalink":"/docs/ai-robotics/autonomous-humanoid-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ai-robotics/autonomous-humanoid-capstone.md","tags":[],"version":"current","frontMatter":{"title":"Capstone - The Autonomous Humanoid","sidebar_label":"Autonomous Humanoid Capstone"},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning","permalink":"/docs/ai-robotics/cognitive-planning"}}');var t=i(4848),o=i(8453);const l={title:"Capstone - The Autonomous Humanoid",sidebar_label:"Autonomous Humanoid Capstone"},r="Capstone: The Autonomous Humanoid",a={},c=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Core Subsystems:",id:"core-subsystems",level:3},{value:"Voice \u2192 Plan Workflows",id:"voice--plan-workflows",level:2},{value:"Example Workflow:",id:"example-workflow",level:3},{value:"Planning Considerations:",id:"planning-considerations",level:3},{value:"Navigation in Dynamic Environments",id:"navigation-in-dynamic-environments",level:2},{value:"Path Planning:",id:"path-planning",level:3},{value:"Environmental Mapping:",id:"environmental-mapping",level:3},{value:"Localization:",id:"localization",level:3},{value:"Perception and Object Recognition",id:"perception-and-object-recognition",level:2},{value:"Vision Processing:",id:"vision-processing",level:3},{value:"Multi-Modal Integration:",id:"multi-modal-integration",level:3},{value:"Recognition Accuracy:",id:"recognition-accuracy",level:3},{value:"Manipulation and Interaction",id:"manipulation-and-interaction",level:2},{value:"Grasp Planning:",id:"grasp-planning",level:3},{value:"Task Execution:",id:"task-execution",level:3},{value:"Human-Robot Interaction:",id:"human-robot-interaction",level:3},{value:"Integration Challenges and Solutions",id:"integration-challenges-and-solutions",level:2},{value:"Challenge: Real-Time Performance",id:"challenge-real-time-performance",level:3},{value:"Challenge: Uncertainty Management",id:"challenge-uncertainty-management",level:3},{value:"Challenge: System Coordination",id:"challenge-system-coordination",level:3},{value:"Challenge: Safety and Reliability",id:"challenge-safety-and-reliability",level:3},{value:"Implementation Strategies",id:"implementation-strategies",level:2},{value:"Modular Architecture:",id:"modular-architecture",level:3},{value:"Simulation to Reality:",id:"simulation-to-reality",level:3},{value:"Performance Optimization:",id:"performance-optimization",level:3},{value:"Evaluation and Validation",id:"evaluation-and-validation",level:2},{value:"Task Success Metrics:",id:"task-success-metrics",level:3},{value:"Human Interaction Metrics:",id:"human-interaction-metrics",level:3},{value:"Robustness Metrics:",id:"robustness-metrics",level:3},{value:"Future Directions",id:"future-directions",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"capstone-the-autonomous-humanoid",children:"Capstone: The Autonomous Humanoid"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"In this capstone project, we integrate all components of the Vision Language Action (VLA) system to create an autonomous humanoid robot capable of understanding natural language commands and executing complex multi-step tasks. This project demonstrates the complete flow from voice input to environmental perception to action execution."}),"\n",(0,t.jsx)(e.p,{children:"The autonomous humanoid system encompasses the entire VLA pipeline:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Voice \u2192 Plan"}),": Natural language command processing with LLM cognitive planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigate"}),": Path planning and execution in dynamic environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perceive"}),": Real-time environmental sensing and object recognition"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manipulate"}),": Physical interaction with objects in the environment"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(e.p,{children:"The complete autonomous humanoid system integrates multiple subsystems in a coherent architecture:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"[Voice Command] \u2192 [Speech Recognition] \u2192 [Intent Processing] \u2192 [LLM Planning] \n     \u2193\n[Environmental Perception] \u2190\u2192 [State Monitoring] \u2192 [Action Execution] \n     \u2193\n[Navigation System] \u2192 [Manipulation System] \u2192 [Feedback Loop]\n"})}),"\n",(0,t.jsx)(e.h3,{id:"core-subsystems",children:"Core Subsystems:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Interface"}),": Processes natural language commands and generates action plans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception System"}),": Real-time environmental sensing and object identification"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation System"}),": Path planning and obstacle avoidance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manipulation System"}),": Physical interaction with objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"State Management"}),": Monitors robot status and environmental changes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Execution Engine"}),": Coordinates all subsystems for task completion"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"voice--plan-workflows",children:"Voice \u2192 Plan Workflows"}),"\n",(0,t.jsx)(e.p,{children:"The voice-to-plan component processes natural language commands and translates them into executable action sequences:"}),"\n",(0,t.jsx)(e.h3,{id:"example-workflow",children:"Example Workflow:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'User Command: "Go to the kitchen, pick up the water bottle from the counter, \nand bring it to me in the living room"\n\nPlanning Breakdown:\n1. [Navigation] Go to kitchen\n2. [Perception] Locate water bottle on counter\n3. [Manipulation] Grasp water bottle\n4. [Navigation] Return to living room\n5. [Manipulation] Deliver water bottle to user\n'})}),"\n",(0,t.jsx)(e.h3,{id:"planning-considerations",children:"Planning Considerations:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Context"}),": Current robot location, object positions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot Capabilities"}),": Reach constraints, payload limits, battery status"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Constraints"}),": Avoiding obstacles, preventing collisions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Priorities"}),": Handling interruptions or higher-priority tasks"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"navigation-in-dynamic-environments",children:"Navigation in Dynamic Environments"}),"\n",(0,t.jsx)(e.p,{children:"The navigation system enables the humanoid robot to move safely through complex, changing environments:"}),"\n",(0,t.jsx)(e.h3,{id:"path-planning",children:"Path Planning:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Global Path Planning"}),": Finds optimal route from start to goal"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Local Path Planning"}),": Adjusts for immediate obstacles and changes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Obstacle Avoidance"}),": Responds to moving objects and people"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"environmental-mapping",children:"Environmental Mapping:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Static Map Construction"}),": Building representation of fixed obstacles"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Object Tracking"}),": Monitoring moving objects and people"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Mapping"}),": Associating locations with object types (kitchen, living room)"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"localization",children:"Localization:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simultaneous Localization and Mapping (SLAM)"}),": Understanding position in environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-Sensor Fusion"}),": Combining camera, LIDAR, and IMU data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Uncertainty Management"}),": Handling sensor noise and ambiguity"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"perception-and-object-recognition",children:"Perception and Object Recognition"}),"\n",(0,t.jsx)(e.p,{children:"The perception system provides real-time environmental awareness and object identification:"}),"\n",(0,t.jsx)(e.h3,{id:"vision-processing",children:"Vision Processing:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Detection"}),": Identifying objects in the robot's field of view"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Classification"}),": Recognizing object types and properties"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Pose Estimation"}),": Determining object positions and orientations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene Understanding"}),": Interpreting spatial relationships"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"multi-modal-integration",children:"Multi-Modal Integration:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RGB-D Perception"}),": Combining color and depth information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Fusion"}),": Integrating camera, depth sensors, and tactile feedback"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Consistency"}),": Tracking objects across multiple time steps"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"recognition-accuracy",children:"Recognition Accuracy:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Confidence Scoring"}),": Assessing reliability of object identifications"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Active Perception"}),": Moving sensors to improve uncertain detections"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context-Based Recognition"}),": Using environmental context for identification"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"manipulation-and-interaction",children:"Manipulation and Interaction"}),"\n",(0,t.jsx)(e.p,{children:"The manipulation system enables the humanoid to physically interact with objects:"}),"\n",(0,t.jsx)(e.h3,{id:"grasp-planning",children:"Grasp Planning:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grasp Selection"}),": Choosing optimal grasp points and orientations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Force Control"}),": Applying appropriate forces for secure grasping"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grasp Adaptation"}),": Adjusting for object properties and uncertainties"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"task-execution",children:"Task Execution:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Kinematic Planning"}),": Calculating joint movements for desired positions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Control"}),": Executing movements with appropriate speed and force"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Failure Recovery"}),": Handling grasp failures and other execution errors"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"human-robot-interaction",children:"Human-Robot Interaction:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Navigation"}),": Moving safely around humans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Collaborative Tasks"}),": Working alongside humans in shared spaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Communication Feedback"}),": Providing status updates and requesting clarification"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"integration-challenges-and-solutions",children:"Integration Challenges and Solutions"}),"\n",(0,t.jsx)(e.p,{children:"Integrating all VLA components presents several significant challenges:"}),"\n",(0,t.jsx)(e.h3,{id:"challenge-real-time-performance",children:"Challenge: Real-Time Performance"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Multiple subsystems requiring real-time processing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Asynchronous processing with priority-based scheduling"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"challenge-uncertainty-management",children:"Challenge: Uncertainty Management"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Uncertain perception, planning, and execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Probabilistic reasoning and belief state tracking"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"challenge-system-coordination",children:"Challenge: System Coordination"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Multiple subsystems operating independently"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Centralized state management and coordination layer"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"challenge-safety-and-reliability",children:"Challenge: Safety and Reliability"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Issue"}),": Ensuring safe operation in human environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Solution"}),": Multiple safety layers, fallback mechanisms, and validation"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"implementation-strategies",children:"Implementation Strategies"}),"\n",(0,t.jsx)(e.p,{children:"The autonomous humanoid system employs several implementation strategies:"}),"\n",(0,t.jsx)(e.h3,{id:"modular-architecture",children:"Modular Architecture:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Component Independence"}),": Subsystems can be developed and tested separately"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Interface Standardization"}),": Clear, well-defined interfaces between components"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reusability"}),": Components can be reused across different robotic platforms"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"simulation-to-reality",children:"Simulation to Reality:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation Testing"}),": Initial testing and validation in simulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reality Gap Mitigation"}),": Techniques to reduce differences between simulation and reality"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Progressive Deployment"}),": Gradually increasing real-world testing complexity"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Parallel Processing"}),": Utilizing multi-core processors effectively"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Specialized Hardware"}),": Using accelerators for perception and planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Efficient Algorithms"}),": Optimized implementations for real-time requirements"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"evaluation-and-validation",children:"Evaluation and Validation"}),"\n",(0,t.jsx)(e.p,{children:"The system's performance is evaluated across multiple dimensions:"}),"\n",(0,t.jsx)(e.h3,{id:"task-success-metrics",children:"Task Success Metrics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Completion Rate"}),": Percentage of tasks successfully completed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Time Efficiency"}),": Time taken to complete tasks relative to optimal"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Path Efficiency"}),": Navigation optimality compared to shortest paths"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"human-interaction-metrics",children:"Human Interaction Metrics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Naturalness"}),": How intuitive the interaction feels to users"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Response Time"}),": Time from command to robot action initiation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Success Rate"}),": Percentage of commands correctly interpreted and executed"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"robustness-metrics",children:"Robustness Metrics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Failure Recovery"}),": Ability to recover from perception or execution failures"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Adaptability"}),": Performance across different environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Long-term Stability"}),": System reliability over extended operation periods"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,t.jsx)(e.p,{children:"The autonomous humanoid system represents a significant milestone, but several areas offer opportunities for advancement:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Learning from Experience"}),": Improving performance through interaction"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multi-Robot Coordination"}),": Collaborative tasks with multiple robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Advanced Reasoning"}),": More sophisticated planning and problem-solving"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Intelligence"}),": Enhanced human-robot interaction capabilities"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"This capstone project demonstrates the integration of language, vision, and action capabilities into a unified system that can understand and execute natural language commands in complex, real-world environments."})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>l,x:()=>r});var s=i(6540);const t={},o=s.createContext(t);function l(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);